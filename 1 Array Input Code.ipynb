{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f98e8733",
   "metadata": {},
   "source": [
    "#### 1) Input > RBM1 > BP > Input > RBM2 > BP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e942b4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee0d6f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBM(nn.Module):\n",
    "    def __init__(self, W, v_bias, h_bias):\n",
    "        super(RBM, self).__init__()\n",
    "        self.W = nn.Parameter(W)\n",
    "        self.v_bias = nn.Parameter(v_bias)\n",
    "        self.h_bias = nn.Parameter(h_bias)\n",
    "\n",
    "    def forward(self, v):\n",
    "        v = v.float()\n",
    "        h = torch.sigmoid(torch.matmul(v, self.W) + self.h_bias)\n",
    "        return h\n",
    "\n",
    "    def backward(self, h):\n",
    "        #h = h.float()\n",
    "        v = torch.sigmoid(torch.matmul(h, self.W.t()) + self.v_bias)\n",
    "        return v\n",
    "    \n",
    "    def contrastive_divergence(self, v):\n",
    "        h = self.forward(v)\n",
    "        v1 = self.backward(h)\n",
    "        print('---------------------------------------------------')\n",
    "        print('v:', v)\n",
    "        print('h:', h)\n",
    "        print('v1:', v1)\n",
    "        #print('---------------------------------------------------')\n",
    "        v = v.float()\n",
    "        h = h.float()\n",
    "        v1 = v1.float()\n",
    "        \n",
    "        v = v.unsqueeze(1)\n",
    "        v1 = v1.unsqueeze(1)\n",
    "        h = h.unsqueeze(0)\n",
    "        \n",
    "        positive_grad = torch.matmul(v, h)\n",
    "        negative_grad = torch.matmul(v1, h)\n",
    "        w_g = positive_grad - negative_grad\n",
    "        \n",
    "        v = v.squeeze(1)\n",
    "        v1 = v1.squeeze(1)\n",
    "        h = h.squeeze(0)\n",
    "        \n",
    "        v_b_g = torch.mean(v - v1)\n",
    "        h_b_g = torch.mean(h - h)\n",
    "        \n",
    "        recon_error = torch.mean(v - v1)**2\n",
    "        return recon_error, w_g, v_b_g, h_b_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7fff08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "def min_max_normalization(data):\n",
    "    min_val = np.min(data)\n",
    "    max_val = np.max(data)\n",
    "    normalized_data = (data - min_val) / (max_val - min_val)\n",
    "    return normalized_data\n",
    "\n",
    "# Sample data\n",
    "data = np.array([50, 30, 15, 20])\n",
    "\n",
    "# Min-Max Normalization\n",
    "normalized_data = min_max_normalization(data)\n",
    "b_normalized_data = torch.tensor(normalized_data)\n",
    "\n",
    "print(b_normalized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efd7679e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.2593, 0.3945, 0.5451], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.4101, 0.2936, 0.3762, 0.3723], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.2624, 0.4003, 0.5534], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.4184, 0.2957, 0.3723, 0.3701], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.2655, 0.4061, 0.5617], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.4269, 0.2979, 0.3682, 0.3679], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.2686, 0.4120, 0.5699], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.4357, 0.3001, 0.3639, 0.3655], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.2717, 0.4178, 0.5781], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.4447, 0.3023, 0.3595, 0.3630], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.2748, 0.4237, 0.5863], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.4539, 0.3045, 0.3549, 0.3604], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.2779, 0.4296, 0.5943], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.4633, 0.3068, 0.3503, 0.3576], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.2811, 0.4355, 0.6023], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.4729, 0.3091, 0.3454, 0.3548], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.2842, 0.4414, 0.6102], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.4827, 0.3114, 0.3405, 0.3519], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.2873, 0.4472, 0.6179], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.4926, 0.3137, 0.3355, 0.3490], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.2904, 0.4531, 0.6256], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.5026, 0.3160, 0.3304, 0.3459], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.2935, 0.4589, 0.6331], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.5127, 0.3183, 0.3252, 0.3428], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.2966, 0.4646, 0.6405], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.5230, 0.3207, 0.3199, 0.3396], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.2997, 0.4704, 0.6478], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.5332, 0.3230, 0.3146, 0.3363], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.3027, 0.4761, 0.6549], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.5436, 0.3254, 0.3092, 0.3330], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.3058, 0.4817, 0.6619], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.5539, 0.3277, 0.3038, 0.3296], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.3088, 0.4872, 0.6687], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.5642, 0.3301, 0.2984, 0.3262], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.3118, 0.4927, 0.6754], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.5744, 0.3324, 0.2929, 0.3227], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.3147, 0.4982, 0.6819], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.5847, 0.3348, 0.2875, 0.3193], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.3176, 0.5035, 0.6882], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.5948, 0.3371, 0.2820, 0.3158], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.3205, 0.5088, 0.6944], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6048, 0.3394, 0.2766, 0.3123], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.3234, 0.5140, 0.7004], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6147, 0.3417, 0.2712, 0.3088], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.3262, 0.5191, 0.7062], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6245, 0.3440, 0.2658, 0.3053], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.3290, 0.5242, 0.7119], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6341, 0.3462, 0.2605, 0.3018], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.3318, 0.5291, 0.7174], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6436, 0.3485, 0.2552, 0.2983], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.3345, 0.5340, 0.7227], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6529, 0.3507, 0.2500, 0.2948], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.3372, 0.5387, 0.7279], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6619, 0.3529, 0.2449, 0.2913], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.3398, 0.5434, 0.7329], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6708, 0.3550, 0.2398, 0.2879], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.3424, 0.5480, 0.7377], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6795, 0.3571, 0.2349, 0.2845], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.3450, 0.5525, 0.7424], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6880, 0.3592, 0.2300, 0.2812], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.3475, 0.5569, 0.7470], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6962, 0.3613, 0.2251, 0.2778], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.3499, 0.5612, 0.7514], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7043, 0.3633, 0.2204, 0.2746], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.3524, 0.5654, 0.7556], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7121, 0.3653, 0.2158, 0.2713], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.3548, 0.5695, 0.7597], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7197, 0.3673, 0.2112, 0.2681], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.3571, 0.5735, 0.7637], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7270, 0.3692, 0.2068, 0.2650], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.3594, 0.5775, 0.7676], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7342, 0.3711, 0.2025, 0.2619], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.3617, 0.5813, 0.7713], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7411, 0.3729, 0.1982, 0.2589], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.3639, 0.5851, 0.7749], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7478, 0.3747, 0.1941, 0.2559], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.3661, 0.5888, 0.7784], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7543, 0.3765, 0.1900, 0.2530], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.3682, 0.5923, 0.7817], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7606, 0.3782, 0.1861, 0.2501], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.3703, 0.5959, 0.7850], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7667, 0.3799, 0.1822, 0.2473], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.3724, 0.5993, 0.7881], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7726, 0.3815, 0.1785, 0.2445], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.3744, 0.6026, 0.7912], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7783, 0.3831, 0.1748, 0.2418], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.3764, 0.6059, 0.7942], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7838, 0.3847, 0.1712, 0.2392], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.3784, 0.6091, 0.7970], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7891, 0.3862, 0.1678, 0.2366], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.3803, 0.6122, 0.7998], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7942, 0.3877, 0.1644, 0.2341], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.3822, 0.6153, 0.8025], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7992, 0.3892, 0.1611, 0.2316], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.3841, 0.6183, 0.8050], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.8040, 0.3906, 0.1579, 0.2292], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.3859, 0.6212, 0.8076], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.8086, 0.3919, 0.1548, 0.2269], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.3877, 0.6240, 0.8100], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.8131, 0.3933, 0.1517, 0.2246], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.3894, 0.6268, 0.8124], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.8174, 0.3946, 0.1488, 0.2223], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.3911, 0.6295, 0.8147], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.8216, 0.3958, 0.1459, 0.2201], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.3928, 0.6322, 0.8169], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.8257, 0.3970, 0.1431, 0.2180], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.3945, 0.6348, 0.8190], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.8296, 0.3982, 0.1404, 0.2159], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.3961, 0.6373, 0.8211], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.8333, 0.3994, 0.1378, 0.2139], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.3977, 0.6398, 0.8231], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.8370, 0.4005, 0.1352, 0.2119], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.3993, 0.6422, 0.8251], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.8405, 0.4016, 0.1327, 0.2100], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.4008, 0.6446, 0.8270], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.8439, 0.4026, 0.1303, 0.2081], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.4024, 0.6469, 0.8289], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.8472, 0.4037, 0.1279, 0.2063], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.4039, 0.6492, 0.8307], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.8504, 0.4046, 0.1257, 0.2045], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.4053, 0.6515, 0.8325], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.8535, 0.4056, 0.1234, 0.2028], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.4068, 0.6536, 0.8342], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.8565, 0.4065, 0.1212, 0.2011], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.4082, 0.6558, 0.8358], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.8594, 0.4074, 0.1191, 0.1995], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.4096, 0.6579, 0.8374], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.8622, 0.4083, 0.1171, 0.1979], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.4110, 0.6599, 0.8390], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.8649, 0.4091, 0.1151, 0.1963], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.4123, 0.6619, 0.8405], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.8676, 0.4099, 0.1131, 0.1948], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.4136, 0.6639, 0.8420], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.8701, 0.4107, 0.1112, 0.1933], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.4150, 0.6658, 0.8435], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.8726, 0.4115, 0.1094, 0.1919], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.4162, 0.6677, 0.8449], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.8750, 0.4122, 0.1076, 0.1905], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.4175, 0.6696, 0.8463], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.8773, 0.4129, 0.1058, 0.1891], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.4188, 0.6714, 0.8476], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.8796, 0.4136, 0.1041, 0.1878], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.4200, 0.6732, 0.8490], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.8818, 0.4143, 0.1025, 0.1865], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.4212, 0.6749, 0.8502], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.8839, 0.4149, 0.1008, 0.1852], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.4224, 0.6766, 0.8515], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.8859, 0.4155, 0.0993, 0.1840], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.4236, 0.6783, 0.8527], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.8879, 0.4161, 0.0977, 0.1828], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.4247, 0.6799, 0.8539], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.8899, 0.4167, 0.0962, 0.1817], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.4258, 0.6816, 0.8551], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.8918, 0.4172, 0.0948, 0.1805], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.4270, 0.6832, 0.8562], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.8936, 0.4178, 0.0934, 0.1794], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.4281, 0.6847, 0.8573], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.8954, 0.4183, 0.0920, 0.1784], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.4292, 0.6863, 0.8584], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.8971, 0.4188, 0.0906, 0.1773], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.4302, 0.6878, 0.8595], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.8988, 0.4192, 0.0893, 0.1763], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.4313, 0.6892, 0.8605], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.9004, 0.4197, 0.0880, 0.1753], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.4323, 0.6907, 0.8615], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.9020, 0.4201, 0.0867, 0.1744], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.4334, 0.6921, 0.8625], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.9036, 0.4206, 0.0855, 0.1734], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.4344, 0.6935, 0.8635], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.9051, 0.4210, 0.0843, 0.1725], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.4354, 0.6949, 0.8644], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.9066, 0.4214, 0.0831, 0.1716], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.4364, 0.6963, 0.8654], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.9080, 0.4218, 0.0820, 0.1708], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.4374, 0.6976, 0.8663], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.9094, 0.4221, 0.0809, 0.1699], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.4383, 0.6989, 0.8672], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.9108, 0.4225, 0.0798, 0.1691], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.4393, 0.7002, 0.8681], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.9121, 0.4228, 0.0787, 0.1683], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.4402, 0.7015, 0.8689], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.9134, 0.4231, 0.0777, 0.1676], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.4411, 0.7028, 0.8698], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.9146, 0.4234, 0.0767, 0.1668], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.4421, 0.7040, 0.8706], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.9159, 0.4237, 0.0757, 0.1661], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.4430, 0.7052, 0.8714], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.9171, 0.4240, 0.0747, 0.1654], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.4438, 0.7064, 0.8722], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.9182, 0.4243, 0.0738, 0.1647], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.4447, 0.7076, 0.8730], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.9194, 0.4246, 0.0728, 0.1640], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.4456, 0.7087, 0.8737], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.9205, 0.4248, 0.0719, 0.1633], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.4465, 0.7099, 0.8745], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.9216, 0.4251, 0.0710, 0.1627], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.4473, 0.7110, 0.8752], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.9227, 0.4253, 0.0702, 0.1621], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n",
      "h: tensor([0.4482, 0.7121, 0.8760], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.9237, 0.4255, 0.0693, 0.1615], grad_fn=<SigmoidBackward0>)\n",
      "========================\n",
      "---------------------------------------------------\n",
      "v: tensor([0.4490, 0.7132, 0.8767], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.2376, 0.5967], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.2720, 0.3445, 0.5686], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.4560, 0.7190, 0.8797], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.2402, 0.6055], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.2793, 0.3543, 0.5788], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.4630, 0.7246, 0.8826], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.2429, 0.6143], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.2866, 0.3643, 0.5888], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.4699, 0.7301, 0.8855], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.2456, 0.6232], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.2940, 0.3744, 0.5987], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.4767, 0.7355, 0.8882], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.2484, 0.6321], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.3016, 0.3846, 0.6085], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.4834, 0.7407, 0.8909], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.2512, 0.6410], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.3092, 0.3949, 0.6182], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.4900, 0.7457, 0.8934], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.2540, 0.6499], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.3169, 0.4053, 0.6277], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.4965, 0.7506, 0.8959], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.2569, 0.6587], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.3246, 0.4158, 0.6370], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.5029, 0.7553, 0.8982], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.2598, 0.6674], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.3324, 0.4264, 0.6462], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.5091, 0.7599, 0.9005], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.2627, 0.6761], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.3402, 0.4370, 0.6552], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.5153, 0.7644, 0.9027], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.2656, 0.6847], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.3481, 0.4476, 0.6640], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.5213, 0.7687, 0.9048], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.2686, 0.6931], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.3560, 0.4582, 0.6727], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.5272, 0.7729, 0.9068], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.2716, 0.7014], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.3639, 0.4688, 0.6811], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.5329, 0.7769, 0.9087], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.2745, 0.7096], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.3718, 0.4793, 0.6893], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.5385, 0.7808, 0.9106], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.2775, 0.7176], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.3797, 0.4898, 0.6973], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.5440, 0.7845, 0.9123], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.2805, 0.7254], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.3876, 0.5002, 0.7051], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.5494, 0.7882, 0.9141], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.2835, 0.7331], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.3955, 0.5106, 0.7126], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.5546, 0.7917, 0.9157], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.2865, 0.7406], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.4033, 0.5208, 0.7200], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.5597, 0.7951, 0.9173], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.2895, 0.7478], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.4110, 0.5308, 0.7271], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.5646, 0.7983, 0.9188], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.2925, 0.7549], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.4187, 0.5407, 0.7340], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.5695, 0.8015, 0.9202], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.2955, 0.7618], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.4263, 0.5505, 0.7407], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.5741, 0.8045, 0.9216], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.2985, 0.7684], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.4339, 0.5601, 0.7471], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.5787, 0.8074, 0.9230], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.3014, 0.7749], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.4413, 0.5695, 0.7534], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.5831, 0.8102, 0.9242], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.3043, 0.7811], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.4486, 0.5787, 0.7594], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.5874, 0.8129, 0.9255], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.3072, 0.7872], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.4559, 0.5877, 0.7652], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.5916, 0.8155, 0.9267], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.3101, 0.7930], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.4630, 0.5965, 0.7709], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.5956, 0.8180, 0.9278], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.3130, 0.7986], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.4700, 0.6051, 0.7763], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.5995, 0.8204, 0.9289], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.3158, 0.8041], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.4769, 0.6135, 0.7815], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.6034, 0.8228, 0.9299], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.3187, 0.8093], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.4837, 0.6217, 0.7866], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.6071, 0.8250, 0.9309], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.3215, 0.8143], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.4904, 0.6296, 0.7914], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.6106, 0.8272, 0.9319], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.3242, 0.8192], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.4969, 0.6373, 0.7961], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.6141, 0.8293, 0.9328], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.3269, 0.8239], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.5032, 0.6448, 0.8006], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.6175, 0.8313, 0.9337], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.3296, 0.8283], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.5095, 0.6521, 0.8050], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.6208, 0.8332, 0.9345], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.3323, 0.8327], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.5156, 0.6591, 0.8092], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.6239, 0.8351, 0.9354], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.3349, 0.8368], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.5216, 0.6660, 0.8132], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.6270, 0.8369, 0.9362], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.3375, 0.8408], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.5274, 0.6726, 0.8171], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.6300, 0.8386, 0.9369], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.3401, 0.8446], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.5331, 0.6790, 0.8208], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.6329, 0.8403, 0.9376], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.3426, 0.8483], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.5386, 0.6852, 0.8244], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.6357, 0.8419, 0.9383], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.3451, 0.8519], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.5440, 0.6913, 0.8279], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.6384, 0.8435, 0.9390], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.3476, 0.8553], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.5493, 0.6971, 0.8312], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.6410, 0.8450, 0.9397], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.3500, 0.8585], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.5545, 0.7027, 0.8344], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.6436, 0.8464, 0.9403], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.3524, 0.8617], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.5595, 0.7081, 0.8376], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.6461, 0.8478, 0.9409], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.3547, 0.8647], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.5644, 0.7134, 0.8405], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.6485, 0.8492, 0.9415], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.3571, 0.8676], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.5691, 0.7185, 0.8434], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.6508, 0.8505, 0.9421], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.3594, 0.8704], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.5737, 0.7234, 0.8462], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.6531, 0.8517, 0.9426], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.3616, 0.8731], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.5782, 0.7282, 0.8489], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.6553, 0.8530, 0.9431], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.3638, 0.8757], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.5826, 0.7328, 0.8515], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.6574, 0.8542, 0.9436], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.3660, 0.8782], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.5869, 0.7372, 0.8540], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.6595, 0.8553, 0.9441], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.3681, 0.8805], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.5910, 0.7415, 0.8564], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.6615, 0.8564, 0.9446], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.3702, 0.8828], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.5951, 0.7457, 0.8588], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.6635, 0.8575, 0.9451], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.3723, 0.8851], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.5990, 0.7497, 0.8610], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.6654, 0.8585, 0.9455], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.3744, 0.8872], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6028, 0.7536, 0.8632], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.6672, 0.8595, 0.9459], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.3764, 0.8892], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6065, 0.7574, 0.8653], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.6690, 0.8605, 0.9463], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.3784, 0.8912], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6101, 0.7610, 0.8674], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.6708, 0.8615, 0.9467], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.3803, 0.8931], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6136, 0.7646, 0.8693], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.6725, 0.8624, 0.9471], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.3822, 0.8950], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6170, 0.7680, 0.8712], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.6741, 0.8633, 0.9475], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.3841, 0.8967], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6203, 0.7713, 0.8731], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.6758, 0.8641, 0.9479], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.3859, 0.8985], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6235, 0.7745, 0.8749], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.6773, 0.8650, 0.9482], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.3878, 0.9001], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6267, 0.7776, 0.8766], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.6788, 0.8658, 0.9486], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.3896, 0.9017], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6297, 0.7806, 0.8783], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.6803, 0.8666, 0.9489], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.3913, 0.9032], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6327, 0.7835, 0.8799], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.6818, 0.8673, 0.9492], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.3931, 0.9047], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6356, 0.7863, 0.8815], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.6832, 0.8681, 0.9495], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.3948, 0.9061], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6384, 0.7890, 0.8830], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.6846, 0.8688, 0.9498], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.3964, 0.9075], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6411, 0.7916, 0.8845], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.6859, 0.8695, 0.9501], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.3981, 0.9089], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6437, 0.7942, 0.8860], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.6872, 0.8702, 0.9504], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.3997, 0.9102], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6463, 0.7967, 0.8874], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.6885, 0.8709, 0.9507], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4013, 0.9114], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6488, 0.7991, 0.8887], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.6897, 0.8715, 0.9510], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4029, 0.9126], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6512, 0.8014, 0.8901], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.6909, 0.8722, 0.9512], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4044, 0.9138], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6536, 0.8037, 0.8913], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.6921, 0.8728, 0.9515], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4059, 0.9149], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6559, 0.8059, 0.8926], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.6932, 0.8734, 0.9517], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4074, 0.9160], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6581, 0.8080, 0.8938], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.6944, 0.8740, 0.9520], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4089, 0.9171], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6603, 0.8101, 0.8950], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.6955, 0.8745, 0.9522], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4103, 0.9181], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6624, 0.8121, 0.8961], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.6965, 0.8751, 0.9524], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4117, 0.9191], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6645, 0.8141, 0.8973], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.6976, 0.8756, 0.9527], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4131, 0.9201], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6665, 0.8160, 0.8983], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.6986, 0.8761, 0.9529], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4145, 0.9211], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6685, 0.8179, 0.8994], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.6996, 0.8766, 0.9531], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4159, 0.9220], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6704, 0.8197, 0.9004], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7006, 0.8771, 0.9533], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4172, 0.9229], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6722, 0.8214, 0.9014], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7015, 0.8776, 0.9535], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4185, 0.9237], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6740, 0.8231, 0.9024], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7024, 0.8781, 0.9537], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4198, 0.9246], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6758, 0.8248, 0.9034], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7033, 0.8786, 0.9539], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4211, 0.9254], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6775, 0.8264, 0.9043], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7042, 0.8790, 0.9541], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4223, 0.9262], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6792, 0.8279, 0.9052], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7051, 0.8795, 0.9542], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4235, 0.9269], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6808, 0.8295, 0.9061], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7059, 0.8799, 0.9544], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4247, 0.9277], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6824, 0.8310, 0.9070], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7068, 0.8803, 0.9546], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4259, 0.9284], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6839, 0.8324, 0.9078], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7076, 0.8807, 0.9548], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4271, 0.9291], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6854, 0.8338, 0.9086], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7084, 0.8811, 0.9549], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4282, 0.9298], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6869, 0.8352, 0.9094], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7091, 0.8815, 0.9551], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4294, 0.9304], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6883, 0.8365, 0.9102], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7099, 0.8819, 0.9553], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4305, 0.9311], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6897, 0.8378, 0.9110], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7107, 0.8823, 0.9554], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4316, 0.9317], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6911, 0.8391, 0.9118], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7114, 0.8827, 0.9556], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4327, 0.9323], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6924, 0.8403, 0.9125], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7121, 0.8830, 0.9557], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4337, 0.9329], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6937, 0.8416, 0.9132], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7128, 0.8834, 0.9558], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4348, 0.9335], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6950, 0.8427, 0.9139], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7135, 0.8837, 0.9560], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4358, 0.9341], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6962, 0.8439, 0.9146], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7141, 0.8841, 0.9561], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4368, 0.9347], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6974, 0.8450, 0.9153], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7148, 0.8844, 0.9563], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4378, 0.9352], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6986, 0.8461, 0.9159], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7154, 0.8847, 0.9564], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4388, 0.9357], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.6997, 0.8472, 0.9166], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7161, 0.8850, 0.9565], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4398, 0.9362], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7008, 0.8482, 0.9172], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7167, 0.8853, 0.9567], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4408, 0.9367], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7019, 0.8493, 0.9178], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7173, 0.8856, 0.9568], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4417, 0.9372], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7030, 0.8503, 0.9184], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7179, 0.8859, 0.9569], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4426, 0.9377], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7040, 0.8512, 0.9190], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7185, 0.8862, 0.9570], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4435, 0.9382], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7050, 0.8522, 0.9196], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7191, 0.8865, 0.9571], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4444, 0.9386], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7060, 0.8531, 0.9202], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7196, 0.8868, 0.9572], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4453, 0.9391], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7070, 0.8540, 0.9207], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7202, 0.8871, 0.9574], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4462, 0.9395], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7079, 0.8549, 0.9213], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7207, 0.8873, 0.9575], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4471, 0.9400], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7089, 0.8558, 0.9218], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7212, 0.8876, 0.9576], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4479, 0.9404], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7098, 0.8566, 0.9223], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7218, 0.8879, 0.9577], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4488, 0.9408], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7106, 0.8575, 0.9228], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7223, 0.8881, 0.9578], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4496, 0.9412], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7115, 0.8583, 0.9233], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7228, 0.8884, 0.9579], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4504, 0.9416], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7123, 0.8591, 0.9238], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7233, 0.8886, 0.9580], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4512, 0.9419], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7132, 0.8599, 0.9243], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7237, 0.8888, 0.9581], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4520, 0.9423], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7140, 0.8606, 0.9248], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7242, 0.8891, 0.9582], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4528, 0.9427], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7147, 0.8614, 0.9252], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7247, 0.8893, 0.9583], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4535, 0.9430], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7155, 0.8621, 0.9257], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7251, 0.8895, 0.9584], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4543, 0.9434], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7163, 0.8628, 0.9262], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7256, 0.8897, 0.9584], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4550, 0.9437], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7170, 0.8635, 0.9266], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7260, 0.8900, 0.9585], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4558, 0.9441], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7177, 0.8642, 0.9270], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7265, 0.8902, 0.9586], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4565, 0.9444], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7184, 0.8649, 0.9275], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7269, 0.8904, 0.9587], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4572, 0.9447], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7191, 0.8655, 0.9279], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7273, 0.8906, 0.9588], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4579, 0.9450], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7198, 0.8662, 0.9283], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7277, 0.8908, 0.9589], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4586, 0.9453], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7204, 0.8668, 0.9287], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7281, 0.8910, 0.9590], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4593, 0.9456], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7211, 0.8674, 0.9291], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7285, 0.8912, 0.9590], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4600, 0.9459], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7217, 0.8680, 0.9295], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7289, 0.8914, 0.9591], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4607, 0.9462], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7223, 0.8686, 0.9299], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7293, 0.8916, 0.9592], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4613, 0.9465], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7229, 0.8692, 0.9302], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7297, 0.8918, 0.9593], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4620, 0.9468], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7235, 0.8698, 0.9306], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7301, 0.8919, 0.9593], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4626, 0.9470], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7241, 0.8703, 0.9310], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7304, 0.8921, 0.9594], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4632, 0.9473], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7247, 0.8709, 0.9313], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7308, 0.8923, 0.9595], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4639, 0.9476], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7252, 0.8714, 0.9317], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7312, 0.8925, 0.9596], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4645, 0.9478], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7258, 0.8720, 0.9320], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7315, 0.8926, 0.9596], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4651, 0.9481], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7263, 0.8725, 0.9324], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7318, 0.8928, 0.9597], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4657, 0.9483], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7268, 0.8730, 0.9327], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7322, 0.8930, 0.9598], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4663, 0.9486], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7273, 0.8735, 0.9330], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7325, 0.8931, 0.9598], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4669, 0.9488], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7278, 0.8740, 0.9334], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7328, 0.8933, 0.9599], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4675, 0.9491], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7283, 0.8745, 0.9337], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7332, 0.8935, 0.9600], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4680, 0.9493], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7288, 0.8749, 0.9340], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7335, 0.8936, 0.9600], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4686, 0.9495], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7293, 0.8754, 0.9343], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7338, 0.8938, 0.9601], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4691, 0.9497], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7297, 0.8759, 0.9346], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7341, 0.8939, 0.9601], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4697, 0.9500], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7302, 0.8763, 0.9349], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7344, 0.8941, 0.9602], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4702, 0.9502], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7306, 0.8768, 0.9352], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7347, 0.8942, 0.9603], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4708, 0.9504], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7311, 0.8772, 0.9355], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7350, 0.8944, 0.9603], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4713, 0.9506], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7315, 0.8776, 0.9358], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7353, 0.8945, 0.9604], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4718, 0.9508], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7319, 0.8780, 0.9361], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7356, 0.8946, 0.9604], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4723, 0.9510], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7323, 0.8784, 0.9364], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7359, 0.8948, 0.9605], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4729, 0.9512], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7327, 0.8788, 0.9366], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7361, 0.8949, 0.9605], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4734, 0.9514], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7331, 0.8792, 0.9369], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7364, 0.8950, 0.9606], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4739, 0.9516], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7335, 0.8796, 0.9372], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7367, 0.8952, 0.9606], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4743, 0.9518], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7339, 0.8800, 0.9374], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7370, 0.8953, 0.9607], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4748, 0.9519], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7343, 0.8804, 0.9377], grad_fn=<SigmoidBackward0>)\n",
      "---------------------------------------------------\n",
      "v: tensor([0.7372, 0.8954, 0.9607], grad_fn=<SigmoidBackward0>)\n",
      "h: tensor([0.4753, 0.9521], grad_fn=<SigmoidBackward0>)\n",
      "v1: tensor([0.7346, 0.8807, 0.9380], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "visible_units = 4  \n",
    "hidden_units_rbm1 = 3\n",
    "hidden_units_rbm2 = 2\n",
    "W1 = torch.randn(visible_units, hidden_units_rbm1)*0.1\n",
    "W2 = torch.randn(hidden_units_rbm1, hidden_units_rbm2)*0.1\n",
    "v_bias = torch.randn(visible_units)\n",
    "h1_bias = torch.randn(hidden_units_rbm1)\n",
    "h2_bias = torch.randn(hidden_units_rbm2)\n",
    "learning_rate = 0.1\n",
    "\n",
    "\n",
    "recon_error_1 = []\n",
    "for i in range(100):\n",
    "    rbm1 = RBM(W1, v_bias, h1_bias)\n",
    "    recon_error, w_g, v_b_g, h_b_g = rbm1.contrastive_divergence(b_normalized_data)\n",
    "    W1 += learning_rate*w_g\n",
    "    v_bias += learning_rate*v_b_g\n",
    "    h1_bias += learning_rate*h_b_g\n",
    "    recon_error_1.append(recon_error)\n",
    "\n",
    "print('========================')\n",
    "#print(recon_error_1)    \n",
    "recon_error_2 = []\n",
    "for j in range(150):\n",
    "    rbm2 = RBM(W2, h1_bias, h2_bias)\n",
    "    h = torch.sigmoid(torch.matmul(b_normalized_data.float(), W1) + h1_bias)\n",
    "    recon_error, w_g, v_b_g, h_b_g = rbm2.contrastive_divergence(h)\n",
    "    W2 += learning_rate*w_g\n",
    "    h1_bias += learning_rate*v_b_g\n",
    "    h2_bias += learning_rate*h_b_g\n",
    "    recon_error_2.append(recon_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cf90b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example tensor list\n",
    "tensor_list = recon_error_1\n",
    "\n",
    "# Convert tensor list to numpy array\n",
    "numpy_array = torch.stack(tensor_list).detach().numpy()\n",
    "\n",
    "# Plot\n",
    "plt.plot(numpy_array.T)  # Transpose the array for proper plotting\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.title('Reconstruction Error over Epochs')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7c0d748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example tensor list\n",
    "tensor_list = recon_error_2\n",
    "\n",
    "# Convert tensor list to numpy array\n",
    "numpy_array = torch.stack(tensor_list).detach().numpy()\n",
    "\n",
    "# Plot\n",
    "plt.plot(numpy_array.T)  # Transpose the array for proper plotting\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.title('Reconstruction Error over Epochs')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc280586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9561, 0.4540, 0.0443, 0.1295], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def regenerate_input(v):\n",
    "    # Generate hidden activations\n",
    "    h1 = torch.sigmoid(torch.matmul(v.float(), W1) + h1_bias)\n",
    "    h2 = torch.sigmoid(torch.matmul(h1, W2) + h2_bias)\n",
    "    h1_recon = torch.sigmoid(torch.matmul(h2, W2.t()) + h1_bias)\n",
    "    v_recon = torch.sigmoid(torch.matmul(h1_recon, W1.t()) + v_bias)\n",
    "    return v_recon\n",
    "\n",
    "# Example usage:\n",
    "regenerated_input = regenerate_input(b_normalized_data)\n",
    "print(regenerated_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27f08a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.10133391399759911\n",
      "Structural Similarity Index (SSIM): 0.36084932478330045\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "def calculate_metrics(original_data, regenerated_data):\n",
    "    # Convert tensors to numpy arrays\n",
    "    original_data_np = original_data.detach().numpy()\n",
    "    regenerated_data_np = regenerated_data.detach().numpy()\n",
    "\n",
    "    # Calculate Mean Squared Error (MSE)\n",
    "    mse = ((original_data_np - regenerated_data_np) ** 2).mean()\n",
    "\n",
    "    # Calculate Structural Similarity Index (SSIM) with appropriate window size\n",
    "    min_side = min(original_data_np.shape[-2:])\n",
    "    win_size = min(7, min_side)  # Ensure window size is smaller than or equal to the smaller side of the images\n",
    "    # Ensure window size is odd\n",
    "    win_size = win_size if win_size % 2 == 1 else win_size - 1\n",
    "    ssim_value = ssim(original_data_np, regenerated_data_np, win_size=win_size, data_range=original_data_np.max() - original_data_np.min())\n",
    "\n",
    "    return mse, ssim_value\n",
    "\n",
    "# Example usage:\n",
    "mse, ssim_value = calculate_metrics(b_normalized_data, regenerated_input)\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Structural Similarity Index (SSIM): {ssim_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47af58de",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45750176",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c081105c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "638a2550",
   "metadata": {},
   "source": [
    "#### 2) Input > RBM1 > RBM2 > Back Propagation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab684a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b0da6225",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBM(nn.Module):\n",
    "    def __init__(self, W1, W2, v_bias, h1_bias, h2_bias):\n",
    "        super(RBM, self).__init__()\n",
    "        self.W1 = nn.Parameter(W1)\n",
    "        self.W2 = nn.Parameter(W2)\n",
    "        self.v_bias = nn.Parameter(v_bias)\n",
    "        self.h1_bias = nn.Parameter(h1_bias)\n",
    "        self.h2_bias = nn.Parameter(h2_bias)\n",
    "\n",
    "    def forward(self, v):\n",
    "        v = v.float()\n",
    "        h1 = torch.sigmoid(torch.matmul(v, self.W1) + self.h1_bias)\n",
    "        h2 = torch.sigmoid(torch.matmul(h1, self.W2) + self.h2_bias)\n",
    "        return h2\n",
    "\n",
    "    def backward(self, h2):\n",
    "        h1 = torch.sigmoid(torch.matmul(h2, self.W2.t()) + self.h1_bias)\n",
    "        v = torch.sigmoid(torch.matmul(h1, self.W1.t()) + self.v_bias)\n",
    "        return v\n",
    "    \n",
    "    def contrastive_divergence(self, v):\n",
    "        h2 = self.forward(v)\n",
    "        v_recon = self.backward(h2)\n",
    "        v = v.float()\n",
    "        h2 = h2.float()\n",
    "        #v1 = v1.float()\n",
    "        \n",
    "        v = v.unsqueeze(1)\n",
    "        v_recon = v_recon.unsqueeze(1)\n",
    "        h2 = h2.unsqueeze(0)\n",
    "        \n",
    "        positive_grad = torch.matmul(v, h2)\n",
    "        negative_grad = torch.matmul(v_recon, h2)\n",
    "        w_g = torch.mean(positive_grad - negative_grad)\n",
    "        \n",
    "        v = v.squeeze(1)\n",
    "        v_recon = v_recon.squeeze(1)\n",
    "        h2 = h2.squeeze(0)\n",
    "        \n",
    "        v_b_g = torch.mean(v - v_recon)\n",
    "        recon_error = torch.mean(v - v_recon)**2\n",
    "\n",
    "        return recon_error, w_g, v_b_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a83af90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 0.4286, 0.0000, 0.1429], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "def min_max_normalization(data):\n",
    "    min_val = np.min(data)\n",
    "    max_val = np.max(data)\n",
    "    normalized_data = (data - min_val) / (max_val - min_val)\n",
    "    return normalized_data\n",
    "\n",
    "# Sample data\n",
    "data = np.array([50, 30, 15, 20])\n",
    "\n",
    "# Min-Max Normalization\n",
    "normalized_data = min_max_normalization(data)\n",
    "b_normalized_data = torch.tensor(normalized_data)\n",
    "\n",
    "print(b_normalized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "54f8f1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "visible_units = 4  \n",
    "hidden_units_rbm1 = 64\n",
    "hidden_units_rbm2 = 128\n",
    "W1 = torch.randn(visible_units, hidden_units_rbm1)*0.1\n",
    "W2 = torch.randn(hidden_units_rbm1, hidden_units_rbm2)*0.1\n",
    "v_bias = torch.randn(visible_units)\n",
    "h1_bias = torch.randn(hidden_units_rbm1)\n",
    "h2_bias = torch.randn(hidden_units_rbm2)\n",
    "learning_rate = 0.1\n",
    "\n",
    "\n",
    "recon_error_list = []\n",
    "for i in range(1000):\n",
    "    rbm = RBM(W1, W2, v_bias, h1_bias, h2_bias)\n",
    "    recon_error, w_g, v_b_g = rbm.contrastive_divergence(b_normalized_data)\n",
    "    W1 += learning_rate*w_g\n",
    "    W2 += learning_rate*w_g\n",
    "    v_bias += learning_rate*v_b_g\n",
    "    h1_bias += learning_rate*v_b_g\n",
    "    h2_bias += learning_rate*v_b_g\n",
    "    recon_error_list.append(recon_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e76c0245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example tensor list\n",
    "tensor_list = recon_error_list\n",
    "\n",
    "# Convert tensor list to numpy array\n",
    "numpy_array = torch.stack(tensor_list).detach().numpy()\n",
    "\n",
    "# Plot\n",
    "plt.plot(numpy_array.T)  # Transpose the array for proper plotting\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.title('Reconstruction Error over Epochs')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "510e8098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4710, 0.3477, 0.1016, 0.6512], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def regenerate_input(v):\n",
    "    # Generate hidden activations\n",
    "    h1 = torch.sigmoid(torch.matmul(v.float(), W1) + h1_bias)\n",
    "    h2 = torch.sigmoid(torch.matmul(h1, W2) + h2_bias)\n",
    "    h1_recon = torch.sigmoid(torch.matmul(h2, W2.t()) + h1_bias)\n",
    "    v_recon = torch.sigmoid(torch.matmul(h1_recon, W1.t()) + v_bias)\n",
    "    return v_recon\n",
    "\n",
    "# Example usage:\n",
    "regenerated_input = regenerate_input(b_normalized_data)\n",
    "print(regenerated_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "738e737f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gklna\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.0011781864194521327\n",
      "Structural Similarity Index (SSIM): 0.9908606184421558\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "def calculate_metrics(original_data, regenerated_data):\n",
    "    # Convert tensors to numpy arrays\n",
    "    original_data_np = original_data.detach().numpy()\n",
    "    regenerated_data_np = regenerated_data.detach().numpy()\n",
    "\n",
    "    # Calculate Mean Squared Error (MSE)\n",
    "    mse = ((original_data_np - regenerated_data_np) ** 2).mean()\n",
    "\n",
    "    # Calculate Structural Similarity Index (SSIM) with appropriate window size\n",
    "    min_side = min(original_data_np.shape[-2:])\n",
    "    win_size = min(7, min_side)  # Ensure window size is smaller than or equal to the smaller side of the images\n",
    "    # Ensure window size is odd\n",
    "    win_size = win_size if win_size % 2 == 1 else win_size - 1\n",
    "    ssim_value = ssim(original_data_np, regenerated_data_np, win_size=win_size, data_range=original_data_np.max() - original_data_np.min())\n",
    "\n",
    "    return mse, ssim_value\n",
    "\n",
    "# Example usage:\n",
    "mse, ssim_value = calculate_metrics(b_normalized_data, regenerated_input)\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Structural Similarity Index (SSIM): {ssim_value}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
